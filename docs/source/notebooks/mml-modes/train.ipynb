{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training mode\n",
    "\n",
    "Task training mode offers flexibility in task training, testing and prediction. By default, nesting and cross-validation are active."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2024-01-17 15:29:36,209\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 0.12.0 on Python 3.8.13 with mode TRAIN.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,209\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-data', 'mml-tags', 'mml-dimensionality', 'mml-inference', 'mml-sql', 'mml-similarity']\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,355\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Pivot task is \u001B[33m\u001B[46m\u001B[1mmml_fake_task\u001B[0m.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,361\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/gitlab/mml/src/mml/core/scripts/schedulers/train_scheduler.py:99: UserWarning: Cross-Validation will store 5 model parameters. To reduce memory consumption you may consider either setting mode.store_parameters=false (which will omit storing the model parameters) or reuse.clean_up.parameters=true (which deletes the model parameters at the end of the experiment.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,361\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.15s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,362\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,366\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,367\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,802\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:36,984\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,011\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,283\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,292\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,292\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,292\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:37,292\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "[\u001B[36m2024-01-17 15:29:37,690\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001B[0m\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:03<00:00,  0.84it/s, v_num=9-36, train/loss=2.360]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.90it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.31it/s, v_num=9-36, train/loss=2.310]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.81it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.39it/s, v_num=9-36, train/loss=2.310][\u001B[36m2024-01-17 15:29:45,203\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.38it/s, v_num=9-36, train/loss=2.310]\r\n",
      "[\u001B[36m2024-01-17 15:29:45,689\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:45,690\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?1\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:45,902\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,029\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,057\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,164\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,172\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,172\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,172\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:46,172\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:01<00:00,  1.96it/s, v_num=9-45, train/loss=2.350]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.95it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.48it/s, v_num=9-45, train/loss=2.280]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 15.38it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.49it/s, v_num=9-45, train/loss=2.280]\u001B[A[\u001B[36m2024-01-17 15:29:51,606\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.47it/s, v_num=9-45, train/loss=2.280]\r\n",
      "[\u001B[36m2024-01-17 15:29:52,102\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?1\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,103\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?2\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,456\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,577\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,603\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,703\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,712\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,712\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,712\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:52,712\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "Epoch 0:  67%|████  | 2/3 [00:01<00:00,  1.71it/s, v_num=9-52, train/loss=2.360][\u001B[36m2024-01-17 15:29:54,479\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:54,482\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\u001B[0m\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:01<00:00,  1.92it/s, v_num=9-52, train/loss=2.340]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.62it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.54it/s, v_num=9-52, train/loss=2.280]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 16.41it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  1.51it/s, v_num=9-52, train/loss=2.280][\u001B[36m2024-01-17 15:29:58,202\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.49it/s, v_num=9-52, train/loss=2.280]\r\n",
      "[\u001B[36m2024-01-17 15:29:58,688\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?2\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:58,690\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?3\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:58,903\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,021\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,047\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,154\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,163\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,163\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,163\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:29:59,163\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:01<00:00,  1.95it/s, v_num=9-58, train/loss=2.370]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.92it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.57it/s, v_num=9-58, train/loss=2.300]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 15.46it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  1.51it/s, v_num=9-58, train/loss=2.300][\u001B[36m2024-01-17 15:30:04,883\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.49it/s, v_num=9-58, train/loss=2.300]\r\n",
      "[\u001B[36m2024-01-17 15:30:05,431\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?3\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,432\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?4\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,643\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,763\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,790\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,898\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,909\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,909\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,909\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:05,909\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:01<00:00,  1.91it/s, v_num=0-05, train/loss=2.380]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.40it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.47it/s, v_num=0-05, train/loss=2.330]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.39it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.48it/s, v_num=0-05, train/loss=2.330][\u001B[36m2024-01-17 15:30:11,459\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.45it/s, v_num=0-05, train/loss=2.330]\r\n",
      "[\u001B[36m2024-01-17 15:30:11,970\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?4\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:11,971\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 15 paths have been created during this run.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:12,081\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:12,081\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m 35.72s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 15:30:12,081\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Return value is 2.3153738498687746.\u001B[0m\r\n"
     ]
    }
   ],
   "source": "!mml train tasks=fake trainer.max_epochs=2 tune.lr=false proj=DEMO",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T15:29:32.389129Z",
     "end_time": "2024-01-17T15:30:13.042113Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can reuse the created models to perform predictions as follows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2024-01-17 19:43:12,136\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 0.12.0 on Python 3.8.13 with mode TRAIN.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,136\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-data', 'mml-tags', 'mml-dimensionality', 'mml-inference', 'mml-sql', 'mml-similarity']\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,283\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Pivot task is \u001B[33m\u001B[46m\u001B[1mmml_fake_task\u001B[0m.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,294\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/gitlab/mml/src/mml/core/scripts/schedulers/train_scheduler.py:99: UserWarning: Cross-Validation will store 5 model parameters. To reduce memory consumption you may consider either setting mode.store_parameters=false (which will omit storing the model parameters) or reuse.clean_up.parameters=true (which deletes the model parameters at the end of the experiment.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,295\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.16s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,296\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,302\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,303\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,303\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 6 matching model storages, used the latest from 2024-01-17 19:22:20.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,755\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,961\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:12,990\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,252\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,260\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,260\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,261\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,261\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:13,338\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?0/model_0005.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:43:13,638\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?0/model_0005.pth\u001B[0m\r\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  1.74it/s]\r\n",
      "[\u001B[36m2024-01-17 19:43:15,146\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,148\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?1\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,148\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 3 matching model storages, used the latest from 2024-01-17 15:29:52.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,373\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,489\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,516\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,618\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,626\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,626\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,626\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,626\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:15,640\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?1/model_0002.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:43:15,923\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?1/model_0002.pth\u001B[0m\r\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.59it/s]\r\n",
      "[\u001B[36m2024-01-17 19:43:17,198\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?1\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,200\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?2\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,200\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 3 matching model storages, used the latest from 2024-01-17 15:29:58.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,425\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,543\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,569\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,816\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,825\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,825\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,825\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,825\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:17,838\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?2/model_0002.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:43:18,108\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?2/model_0002.pth\u001B[0m\r\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.53it/s]\r\n",
      "[\u001B[36m2024-01-17 19:43:19,431\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?2\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,432\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?3\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,432\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 3 matching model storages, used the latest from 2024-01-17 15:30:05.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,654\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,772\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,796\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,903\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,912\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,912\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,913\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,913\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:19,929\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?3/model_0002.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:43:20,196\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?3/model_0002.pth\u001B[0m\r\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.60it/s]\r\n",
      "[\u001B[36m2024-01-17 19:43:21,442\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?3\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,443\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?4\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,444\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 3 matching model storages, used the latest from 2024-01-17 15:30:11.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,657\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,774\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,802\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,905\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,913\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,913\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,913\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,914\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:21,933\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?4/model_0002.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:43:22,201\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO/PARAMETERS/mml_fake_task+nested?4/model_0002.pth\u001B[0m\r\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  2.54it/s]\r\n",
      "[\u001B[36m2024-01-17 19:43:23,467\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished predicting for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?4\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,468\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,469\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,469\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 10 paths have been created during this run.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,469\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,469\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m 11.17s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:43:23,470\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Return value is nan.\u001B[0m\r\n"
     ]
    }
   ],
   "source": "!mml train tasks=fake mode.subroutines=[predict] proj=DEMO reuse.models=DEMO",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T19:43:08.389993Z",
     "end_time": "2024-01-17T19:43:24.336497Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also training and testing is possible. Testing does not support CV though!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001B[36m2024-01-17 19:45:23,134\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Started MML 0.12.0 on Python 3.8.13 with mode TRAIN.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,134\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Plugins loaded: ['mml-data', 'mml-tags', 'mml-dimensionality', 'mml-inference', 'mml-sql', 'mml-similarity']\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,285\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Pivot task is \u001B[33m\u001B[46m\u001B[1mmml_fake_task\u001B[0m.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,291\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/Documents/development/gitlab/mml/src/mml/core/scripts/schedulers/train_scheduler.py:107: UserWarning: Chose mode.nested=true so the testing subroutine will be performed NOT on the (potential) official task test split, but on the hold-out fold.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,292\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML init time was 0.0h 0.0m  0.16s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,293\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Preparing experiment ...\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,294\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting experiment!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,295\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,741\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,955\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:23,979\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:24,232\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:24,240\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:24,240\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:24,240\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:24,240\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name          | Type             | Params\r\n",
      "---------------------------------------------------\r\n",
      "0 | model         | TimmGenericModel | 21.3 M\r\n",
      "1 | criteria      | ModuleDict       | 0     \r\n",
      "2 | train_metrics | ModuleDict       | 0     \r\n",
      "3 | val_metrics   | ModuleDict       | 0     \r\n",
      "4 | test_metrics  | ModuleDict       | 0     \r\n",
      "5 | train_cms     | ModuleDict       | 0     \r\n",
      "6 | val_cms       | ModuleDict       | 0     \r\n",
      "7 | test_cms      | ModuleDict       | 0     \r\n",
      "---------------------------------------------------\r\n",
      "21.3 M    Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "21.3 M    Total params\r\n",
      "85.159    Total estimated model params size (MB)\r\n",
      "[\u001B[36m2024-01-17 19:45:24,662\u001B[0m][\u001B[34mpy.warnings\u001B[0m][\u001B[33mWARNING\u001B[0m] - /home/scholzpa/miniconda3/envs/mml/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001B[0m\r\n",
      "Epoch 0: 100%|██████| 3/3 [00:03<00:00,  0.81it/s, v_num=5-23, train/loss=2.360]\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00,  3.01it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:01<00:00,  2.40it/s, v_num=5-23, train/loss=2.310]\u001B[A\r\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001B[A\r\n",
      "Validation DataLoader 0: 100%|████████████████████| 1/1 [00:00<00:00, 14.96it/s]\u001B[A\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.44it/s, v_num=5-23, train/loss=2.310]\u001B[A[\u001B[36m2024-01-17 19:45:32,217\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - `Trainer.fit` stopped: `max_epochs=2` reached.\u001B[0m\r\n",
      "Epoch 1: 100%|██████| 3/3 [00:02<00:00,  1.42it/s, v_num=5-23, train/loss=2.310]\r\n",
      "[\u001B[36m2024-01-17 19:45:32,699\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished training for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m and fold 0.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:32,701\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Starting testing for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:32,701\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Found 1 matching model storages, used the latest from 2024-01-17 19:45:32.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:32,921\u001B[0m][\u001B[34mtimm.models._builder\u001B[0m][\u001B[32mINFO\u001B[0m] - Loading pretrained weights from Hugging Face hub (timm/resnet34.a1_in1k)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,040\u001B[0m][\u001B[34mtimm.models._hub\u001B[0m][\u001B[32mINFO\u001B[0m] - [timm/resnet34.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,066\u001B[0m][\u001B[34mmml.core.models.lightning_single_frame\u001B[0m][\u001B[32mINFO\u001B[0m] - Since sampling is unbalanced will try to auto activate loss weights for classes.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,171\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Using 16bit Automatic Mixed Precision (AMP)\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,180\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - GPU available: True (cuda), used: True\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,180\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - TPU available: False, using: 0 TPU cores\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,180\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - IPU available: False, using: 0 IPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,180\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - HPU available: False, using: 0 HPUs\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:33,190\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Restoring states from the checkpoint path at /home/scholzpa/Documents/exp/mml_results/DEMO2/PARAMETERS/mml_fake_task+nested?0/model_0001.pth\u001B[0m\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "[\u001B[36m2024-01-17 19:45:33,430\u001B[0m][\u001B[34mlightning_fabric.utilities.rank_zero\u001B[0m][\u001B[32mINFO\u001B[0m] - Loaded model weights from the checkpoint at /home/scholzpa/Documents/exp/mml_results/DEMO2/PARAMETERS/mml_fake_task+nested?0/model_0001.pth\u001B[0m\r\n",
      "Testing DataLoader 0: 100%|███████████████████████| 1/1 [00:00<00:00,  1.51it/s]\r\n",
      "────────────────────────────────────────────────────────────────────────────────\r\n",
      "              Test metric                             DataLoader 0\r\n",
      "────────────────────────────────────────────────────────────────────────────────\r\n",
      "               test/loss                              2.314453125\r\n",
      "test/mml_fake_task+nested?0/MulticlassAU           0.5028374791145325\r\n",
      "                ROC_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassAU          0.015444174408912659\r\n",
      "                ROC_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassAc           0.0963830053806305\r\n",
      "              curacy_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassAc          0.004115646705031395\r\n",
      "               curacy_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassAv          0.13723304867744446\r\n",
      "          eragePrecision_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassAv          0.014806008897721767\r\n",
      "           eragePrecision_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassCa          0.04819680005311966\r\n",
      "          librationError_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassCa          0.020869700238108635\r\n",
      "           librationError_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassF1          0.016199154779314995\r\n",
      "               Score_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassF1          0.003188840113580227\r\n",
      "               Score_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassMa          -0.04116348177194595\r\n",
      "          tthewsCorrCoef_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassMa          0.039030902087688446\r\n",
      "           tthewsCorrCoef_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassPr          0.009331938810646534\r\n",
      "              ecision_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassPr         0.0018178964965045452\r\n",
      "              ecision_std\r\n",
      "test/mml_fake_task+nested?0/MulticlassRe          0.09479976445436478\r\n",
      "               call_mean\r\n",
      "test/mml_fake_task+nested?0/MulticlassRe          0.004909082315862179\r\n",
      "                call_std\r\n",
      "    test/mml_fake_task+nested?0/loss               2.3151323795318604\r\n",
      "────────────────────────────────────────────────────────────────────────────────\r\n",
      "[\u001B[36m2024-01-17 19:45:34,969\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Results: [{'test/mml_fake_task+nested?0/loss': 2.3151323795318604, 'test/loss': 2.314453125}]\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:34,969\u001B[0m][\u001B[34mmml.core.scripts.schedulers.train_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Finished testing for task \u001B[33m\u001B[46m\u001B[1mmml_fake_task+nested?0\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:34,970\u001B[0m][\u001B[34mmml.core.data_loading.file_manager\u001B[0m][\u001B[32mINFO\u001B[0m] - A total of 3 paths have been created during this run.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:34,996\u001B[0m][\u001B[34mmml.core.scripts.schedulers.base_scheduler\u001B[0m][\u001B[32mINFO\u001B[0m] - Successfully finished all experiments!\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:34,996\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - MML run time was 0.0h 0.0m 11.70s.\u001B[0m\r\n",
      "[\u001B[36m2024-01-17 19:45:34,996\u001B[0m][\u001B[34mmml\u001B[0m][\u001B[32mINFO\u001B[0m] - Return value is 2.3151772022247314.\u001B[0m\r\n"
     ]
    }
   ],
   "source": "!mml train tasks=fake mode.subroutines=[train,test] proj=DEMO2 trainer.max_epochs=2 tune.lr=false mode.cv=false",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T19:45:19.333340Z",
     "end_time": "2024-01-17T19:45:35.948010Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect any logged results with tensorboard:\n",
    ">>> tensorboard --logdir path/to/MML_RESULTS/DEMO2\n",
    ">>> Open browser -> http://localhost:6006\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
